version: '3'

services:
  embedding-inference:
    image: michaelf34/infinity:latest
    command:
      - --model-name-or-path=thenlper/gte-large-zh
      - --port=8080
      - --batch-size=1024
      - --log-level=info
    deploy:
      resources:
        reservations:
          # Deploy with docker swarm
          generic_resources:
            - discrete_resource_spec:
                kind: "NVIDIA-GPU"
                value: 1
          # Local deployment
          # devices:
          # - driver: nvidia
          #   count: 1
          #   capabilities: [gpu]
    volumes:
      - cache:/app/.cache/torch
    # ports:
      # - 8181:8080
  
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:1.3
    shm_size: '1gb'
    command:
      - --model-id=meta-llama/LlamaGuard-7b
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          # Deploy with docker swarm
          generic_resources:
            - discrete_resource_spec:
                kind: "NVIDIA-GPU"
                value: 1
          # Local deployment
          # devices:
          # - driver: nvidia
          #   count: 1
          #   capabilities: [gpu]
    volumes:
      - cache:/data
    ports:
      - 8182:80
  
  db:
    image: postgres
    restart: always
    environment:
      POSTGRES_PASSWORD: app
      POSTGRES_USER: app
    volumes:
      - db_data:/var/lib/postgresql
    ports:
      - 5432:5432

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
  
  manager:
    image: safety-guard-manager:latest
    build:
      context: .
      dockerfile: manager.Dockerfile
    environment:
      EMBED_HOST: http://embedding-inference:8080
      EMBED_MODEL: thenlper/gte-large-zh
      DB_CONN: postgresql+psycopg2://app:app@db/app
      SERVER_HOST: 0.0.0.0
      SERVER_PORT: "8000"
    ports:
      - 8000:8000

volumes:
  cache:
  db_data:

networks:
  taide:
    name: taide
    external: true