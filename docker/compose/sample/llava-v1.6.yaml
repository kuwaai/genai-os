services:
  llava-v1.6-executor:
    build:
      context: ../../
      dockerfile: docker/executor/Dockerfile
    image: kuwa-executor
    environment:
      EXECUTOR_TYPE: huggingface
      EXECUTOR_ACCESS_CODE: llava-v1.6-7b
      EXECUTOR_NAME: LLaVA v1.6 7B
      EXECUTOR_IMAGE: llava.png # Refer to src/multi-chat/public/images
      # HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
    depends_on:
      - kernel
      - multi-chat
    command: ["--model_path", "llava-hf/llava-v1.6-mistral-7b-hf", "--log", "debug"]
    restart: unless-stopped
    volumes: ["~/.cache/huggingface:/root/.cache/huggingface"]
    networks: ["backend"]
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #       - driver: nvidia
    #         device_ids: ['0']
    #         capabilities: [gpu]