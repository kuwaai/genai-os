version: '3'

services:
  api_doc_qa:
    runtime: nvidia
    build: .
    devices:
      - /dev/nvidia0:/dev/nvidia0
    restart: always
    env_file: .env.dev
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - AGENT_ENDPOINT=http://web_agent:9000/v1.0/
      - LLM_NAME=doc_qa
      - PUBLIC_ADDRESS=api_doc_qa
      - PORT=9001 # Leave blank will assign by the OS.
      - IGNORE_AGENT=False
      - DEBUG=True
      - LAYOUT_CONFIG=${LAYOUT_CONFIG:-layout.yaml}
      - HTTP_CACHE_PROXY=http://web_cache:10250
    networks:
      - taide_backend
      - squid_backend
      - monitor_backend
    healthcheck:
      test: "curl --fail http://localhost:9001/health || exit 1"
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - /var/models/llama2-7b-chat-b5.0.0:/llm/:ro
      - log:/var/log/doc_qa

# Connect to the network created by web_agent and squid
networks:
  taide_backend:
    external: true
  squid_backend:
    external: true
  monitor_backend:
    external: true
volumes:
  log: