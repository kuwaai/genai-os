#!/usr/bin/env python
# coding: utf-8

import torch, accelerate, time
import chevron
import logging
import asyncio
import re
from functools import reduce
from pathlib import Path
from dataclasses import dataclass
from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, \
                         StoppingCriteria, StoppingCriteriaList, \
                         pipeline, GenerationConfig

from worker_framework.datatype import ChatRecord, Role

@dataclass
class ChatTuple:
    """
    Grouped chat record for rendering prompt.
    """
    system:str = None
    user:str = None
    bot:str = None

class ChatTupleFactory:
    
    @staticmethod
    def from_chat_history(chat_history: [ChatRecord]) -> [ChatTuple]:
        """
        Convert a list of ChatRecords to a list o ChatTuple.
        Noticed that it's expected that a bot message is an immediate successor of a user message.
        """
        chat_tuples = []
        for chat in chat_history:
            if chat.role == Role.USER:
                chat_tuples.append(ChatTuple(user=chat.msg))
            elif chat.role == Role.BOT:
                chat_tuples[-1].bot = chat.msg
        return chat_tuples

class StopOnTokens(StoppingCriteria):
    def __init__(self, tokenizer: AutoTokenizer):
        stop_list = ['<s>', '</s>', '[INST]', '\nQuestion:', "[INST: ]"]
        to_token_id = lambda x: torch.LongTensor(tokenizer(x)['input_ids']).to('cuda')
        self.stop_token_ids = map(to_token_id, stop_list)
    
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_ids in self.stop_token_ids:
            if torch.all(input_ids[0][-len(stop_ids):] == stop_ids):
                return True
        return False

class TaideLlm:
    def __init__(self,
                 token_limit = 3500,
                 model_path = '/llm/llama2-7b-chat-b5.0.0',
                 prompt_template_path = 'prompt_template/taide.mustache'):
        self.logger = logging.getLogger(__name__)
        
        self.input_token_limit = token_limit

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            config=AutoConfig.from_pretrained(model_path),
            device_map="auto",
            torch_dtype=torch.float16
        )
        model.eval()

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

        self.pipe = pipeline(
            model=model,
            tokenizer=self.tokenizer,
            return_full_text=False,
            task='text-generation',
            stopping_criteria=StoppingCriteriaList([StopOnTokens(self.tokenizer)]),
            max_length=4096,
            # max_new_tokens=2048,
            # num_beams=2, early_stopping=True, # Beam search
            do_sample=True, temperature=0.2, top_p=0.95, # Top-p (nucleus) sampling
            # penalty_alpha=0.6, top_k=3, low_memory=True, # Contrastive search
            repetition_penalty=1.0,
        )

        prompt_template_file = Path(prompt_template_path)
        self.prompt_template = prompt_template_file.read_text()

    def is_too_long(self, chat_history: [ChatRecord]) -> bool:
        """
        Estimate whether the prompt generated by the given chat history will be too long.
        This public API can be use to evaluate how many documents can be placed in the context window.
        """
        
        chat_history = ChatTupleFactory.from_chat_history(chat_history)
        prompt = self.gen_prompt(chat_history)
        return self._is_too_long(prompt)[0]
    
    def _is_too_long(self, sentence: str) -> bool:
        """
        Calculate whether the number of tokens of given sentence exceeds the threshold.
        """

        num_tokens = len(self.tokenizer.tokenize(sentence))
        return num_tokens >= self.input_token_limit, num_tokens

    def gen_prompt(self, chat_history: [ChatTuple], append_system: bool = True) -> str:
        """
        Generate prompt from given chat history.
        """

        system_chat_tuple = ChatTuple(
            system = 'You are a helpful assistant. 你是一個樂於助人的助手。',
            user = '請用中文回答我',
            # bot = '好! 我樂於助人,是你的好助手。' # b1.0.0
            bot = '當然!為方便溝通,我使用的是傳統中文語言。您有何請求或疑問,請慷慨請教我?'
        )

        if append_system:
            chat_history = [system_chat_tuple] + chat_history

        prompt = chevron.render(
            self.prompt_template,
            {'history': chat_history}
        )

        return prompt

    async def complete(self, chat_history: [ChatRecord]): 
        result = ''
        try:
            
            chat_history = ChatTupleFactory.from_chat_history(chat_history)
            self.logger.info('Data: {}'.format(chat_history))
            
            # Trim the over-length history
            prompt = ''
            tokens = 0
            while True:
                prompt = self.gen_prompt(chat_history)
                too_long, tokens = self._is_too_long(prompt)
                if not too_long: break
                chat_history = chat_history[1:]
            
            self.logger.info('Final Prompt ({} tokens):\n{}'.format(tokens, prompt))
            
            self.logger.info('Generating...')
            loop = asyncio.get_running_loop()
            result = await loop.run_in_executor(None, self.pipe, prompt)
            result = result[0]['generated_text']
            output_tokens = len(self.tokenizer.tokenize(result))
            self.logger.info(f'Generation finished. Generated {output_tokens} tokens.')
            self.logger.debug(f'Reply: {result}')
            
        except Exception as e:
            result = ''
            self.logger.exception('Generation failed.')
        finally:
            torch.cuda.empty_cache()
            return result
